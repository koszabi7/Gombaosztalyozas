{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMYzMnO4pZcydgffsS075Uz"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[" Import Necessary Libraries"],"metadata":{"id":"hkOqNL6okDHm"}},{"cell_type":"code","execution_count":1,"metadata":{"id":"e1BE4LUQT23H","colab":{"base_uri":"https://localhost:8080/","height":356},"collapsed":true,"executionInfo":{"status":"error","timestamp":1733337517924,"user_tz":-60,"elapsed":3233,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}},"outputId":"70b4ce98-e24b-4471-c5a2-cfc5c01296dd"},"outputs":[{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-1-879d52ece3a7>\u001b[0m in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpreprocessing\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mimg_to_array\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mload_img\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapplications\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresnet50\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mResNet50\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_input\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGlobalAveragePooling2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0m_tf2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__internal__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__operators__\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maudio\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdistribute\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0meager_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 13\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_api\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mv2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__internal__\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mgraph_util\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/_api/v2/__internal__/feature_column/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0m_sys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDenseColumn\u001b[0m \u001b[0;31m# line: 1777\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mFeatureTransformationCache\u001b[0m \u001b[0;31m# line: 1962\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column_v2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mSequenceDenseColumn\u001b[0m \u001b[0;31m# line: 1941\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/feature_column/feature_column_v2.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mreaders\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meager\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfc_old\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfeature_column_v2_types\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfc_types\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_column\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialization\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/feature_column/feature_column.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msparse_tensor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msparse_tensor_lib\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mops\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0marray_ops_stack\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/layers/base.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;31m# =============================================================================\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\"\"\"Contains the base Layer class, from which all layers inherit.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlegacy_tf_layers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mInputSpec\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mInputSpec\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m# See b/110718070#comment18 for more details about this import.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmodels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minput_layer\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mInput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/models.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mmetrics_module\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0moptimizer_v1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfunctional\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_conversion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensor_shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mactivations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbase_layer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/activations.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mbackend\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0madvanced_activations\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdeserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgeneric_utils\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mserialize_keras_object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/layers/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;31m# Convolution layers.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv1D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv2D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpython\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvolutional\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mConv3D\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/keras/layers/convolutional.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m   2477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2478\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2479\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mUpSampling1D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mLayer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2480\u001b[0m   \"\"\"Upsampling layer for 1D inputs.\n\u001b[1;32m   2481\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# Import necessary libraries\n","import zipfile\n","import os\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","source":["import os\n","from PIL import Image, ImageFile\n","from torchvision import datasets, transforms, models\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torch import nn, optim\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","import numpy as np\n","import seaborn as sns\n","import pandas as pd"],"metadata":{"id":"N7AnhifpNjAp","executionInfo":{"status":"aborted","timestamp":1733337517926,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Mount Google Drive and Extract ZIP file"],"metadata":{"id":"b3jUzaEqkIUr"}},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the path to the ZIP file in your Google Drive\n","zip_file_path = '/content/drive/MyDrive/ADATELEMZÉS_HF/top_100_species_images.zip'"],"metadata":{"id":"QqOWGlRXXkXR","executionInfo":{"status":"aborted","timestamp":1733337517926,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Load Images and Extract Labels"],"metadata":{"id":"gceJK-Bfk6HU"}},{"cell_type":"code","source":["#### Load the dataset from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/ADATELEMZÉS_HF/FungiCLEF2023_train_metadata_PRODUCTION.csv')\n","\n","#### Find the top 100 species by number of examples\n","top_100_species = df['species'].value_counts().head(100).index.tolist()\n","\n","#### Filter the dataframe to only include rows from the top 100 species\n","df_top_100 = df[df['species'].isin(top_100_species)]\n","\n","#### Create a new dataframe with only the 'image_path' and 'class_id' columns\n","top_100_pairs = df_top_100[['image_path', 'species']]\n","\n","#### Print the shape of the final dataframe\n","print(f\"Final dataframe shape: {top_100_pairs.shape}\")\n","\n","unique_species_ids = top_100_pairs['species'].nunique()\n","print(f\"Number of unique species: {unique_species_ids}\")\n","print(top_100_species)\n","\n","# Create a dictionary that maps each species to a unique integer label\n","species_labels = {species: index for index, species in enumerate(top_100_species)}\n","\n","print(species_labels)"],"metadata":{"id":"HvT91vRCaJxA","executionInfo":{"status":"aborted","timestamp":1733337517927,"user_tz":-60,"elapsed":13,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","# Define transformations to be applied to each image (resizing, converting to tensor, normalizing)\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),  # Resize images to 288x288 pixels\n","    transforms.ToTensor(),  # Convert image to a tensor (used by PyTorch)\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"Z5mfW2PqTQyz","executionInfo":{"status":"aborted","timestamp":1733337517927,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import zipfile\n","\n","# Define a custom dataset class for loading our images and labels\n","class CustomDataset(Dataset):\n","    def __init__(self, zip_file_path, top_100_df, transform=None):\n","        self.zip_file_path = zip_file_path\n","        self.top_100_df = top_100_df\n","        self.transform = transform\n","        self.image_paths = []  # List to store image paths\n","        self.labels = []       # List to store image labels\n","        image_count = 0\n","        label_count = 0\n","        iter = 0\n","\n","        print(\"Zip folder path:\", zip_file_path)  # Print the folder path\n","\n","        # Open the ZIP file\n","        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n","            # Get the name of the folder\n","            folder_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n","            print(\"Folder name:\", folder_name)  # Print the subfolder name\n","\n","            # Loop through each file in the ZIP file\n","            for file_info in zip_file.infolist():\n","                file_name = file_info.filename\n","                # print(\"File name:\", file_name)  # Print the file name\n","\n","                if file_name.startswith(folder_name + '/') and file_name.endswith('.JPG') and not file_name.startswith('__MACOSX/'):\n","                    #print(\"Matched file:\", file_name)  # Print the matched file name\n","\n","                    if(iter % 10 == 0):\n","                        # Get the image path after the '/' symbol\n","                        image_path_after_slash = os.path.basename(file_name)\n","                        #print(\"Needed name: \", image_path_after_slash)\n","\n","                        # Extract the label from the file name\n","                        label = species_labels.get(self.top_100_df.loc[self.top_100_df['image_path'] == image_path_after_slash, 'species'].values[0])\n","                        # print(\"Extracted label:\", label)  # Print the extracted label\n","\n","                        self.image_paths.append(file_name)  # Store the image path within the ZIP file\n","                        image_count += 1\n","                        self.labels.append(label)  # Store the corresponding label\n","                        label_count += 1\n","\n","                    iter = iter + 1\n","\n","                    if(label_count % 100 == 0):\n","                      print(label_count)\n","\n","        # print(\"Image paths:\", self.image_paths)\n","        # print(\"Labels:\", self.labels)\n","        print(len(set(self.labels)))\n","        print(\"ZIP file path:\", zip_file_path)  # Print the ZIP file path\n","        print(\"Subfolder name:\", folder_name)  # Print the subfolder name\n","        print(\"Image count:\", image_count)\n","        print(\"Label count:\", label_count)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Open the ZIP file and read the image\n","        with zipfile.ZipFile(self.zip_file_path, 'r') as zip_file:\n","            img_path = self.image_paths[idx]\n","            image = Image.open(zip_file.open(img_path)).convert(\"RGB\")\n","            label = self.labels[idx]\n","\n","            if self.transform:\n","                image = self.transform(image)\n","\n","        return image, label"],"metadata":{"id":"Xd1pwuwfN5lJ","executionInfo":{"status":"aborted","timestamp":1733337517927,"user_tz":-60,"elapsed":11,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create dataset and dataloader objects for the training dataset\n","dataset = CustomDataset(zip_file_path=zip_file_path, top_100_df=top_100_pairs, transform=transform)\n","\n","# Create DataLoaders to efficiently load data in batches\n","train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"],"metadata":{"id":"b-eMPx85jdGJ","executionInfo":{"status":"aborted","timestamp":1733337517927,"user_tz":-60,"elapsed":11,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define the model using pre-trained ResNet-50\n","class MushroomClassifier(nn.Module):\n","    def __init__(self, num_classes=100):\n","        super(MushroomClassifier, self).__init__()\n","        self.model = models.resnet50(pretrained=True)\n","        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Instantiate the model and move it to GPU if available\n","num_classes = 100\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = MushroomClassifier(num_classes=num_classes).to(device)"],"metadata":{"id":"t2h5IIMzhdPA","executionInfo":{"status":"aborted","timestamp":1733337517927,"user_tz":-60,"elapsed":11,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"id":"Fp-yMVNAhoDr","executionInfo":{"status":"aborted","timestamp":1733337517928,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# KFold keresztvalidációs trainelése a modellnek"],"metadata":{"id":"9BiWuybTpH0z"}},{"cell_type":"code","source":["\n","from sklearn.model_selection import KFold\n","from torch.utils.data import DataLoader, SubsetRandomSampler\n","\n","# Function to reset model weights\n","def reset_weights(m):\n","    if hasattr(m, 'reset_parameters'):\n","        m.reset_parameters()\n","\n","# Define the train_kfold function\n","def train_kfold(model, dataset, batch_size, num_folds, num_epochs, optimizer, criterion, device):\n","    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","\n","    train_losses, val_losses, train_accuracies, val_accuracies = [], [], [], []\n","\n","    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n","        print(f\"Fold {fold + 1}/{num_folds}\")\n","\n","        train_sampler = SubsetRandomSampler(train_idx)\n","        val_sampler = SubsetRandomSampler(val_idx)\n","\n","        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n","        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n","\n","        model.apply(reset_weights)\n","\n","        for epoch in range(num_epochs):\n","            print(\"Itt vagyok epochban:\", epoch + 1)\n","            model.train()\n","            train_loss, train_acc = 0.0, 0.0\n","\n","            for batch in train_loader:\n","                print(\"Itt vagyok batchben:\", batch + 1)\n","                inputs, targets = batch\n","                inputs, targets = inputs.to(device), targets.to(device)\n","\n","                optimizer.zero_grad()\n","                outputs = model(inputs)\n","                loss = criterion(outputs, targets)\n","                loss.backward()\n","                optimizer.step()\n","\n","                train_loss += loss.item()\n","                _, preds = torch.max(outputs, 1)\n","                train_acc += torch.sum(preds == targets).item()\n","\n","            train_loss /= len(train_loader)\n","            train_acc /= len(train_idx)\n","\n","            model.eval()\n","            val_loss, val_acc = 0.0, 0.0\n","\n","            with torch.no_grad():\n","                for batch in val_loader:\n","                    inputs, targets = batch\n","                    inputs, targets = inputs.to(device), targets.to(device)\n","\n","                    outputs = model(inputs)\n","                    loss = criterion(outputs, targets)\n","                    val_loss += loss.item()\n","\n","                    _, preds = torch.max(outputs, 1)\n","                    val_acc += torch.sum(preds == targets).item()\n","\n","            val_loss /= len(val_loader)\n","            val_acc /= len(val_idx)\n","\n","            print(f\"Epoch {epoch + 1}/{num_epochs} | \"\n","                  f\"Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, \"\n","                  f\"Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","            train_losses.append(train_loss)\n","            val_losses.append(val_loss)\n","            train_accuracies.append(train_acc)\n","            val_accuracies.append(val_acc)\n","\n","    return train_losses, val_losses, train_accuracies, val_accuracies\n"],"metadata":{"id":"aPMNQazzpTU4","executionInfo":{"status":"aborted","timestamp":1733337517928,"user_tz":-60,"elapsed":11,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_losses, val_losses, train_accuracies, val_accuracies = train_kfold(\n","    model=model,\n","    dataset=dataset,\n","    batch_size= 16,\n","    num_folds= 5,\n","    num_epochs= 10,\n","    optimizer=optimizer,\n","    criterion=criterion,\n","    device=device\n",")"],"metadata":{"id":"LPzg8ClytEmI","executionInfo":{"status":"aborted","timestamp":1733337517928,"user_tz":-60,"elapsed":11,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score, accuracy_score, classification_report\n","\n","y_pred = model.predict(X_test)\n","\n","y_pred_classes = y_pred.argmax(axis=1)\n","\n","macro_f1 = f1_score(y_test, y_pred_classes, average='macro')\n","print(f\"Macro F1 score: {macro_f1:.4f}\")\n","\n","accuracy = accuracy_score(y_test, y_pred_classes)\n","print(f\"Accuracy: {accuracy:.4f}\")\n","\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_classes))\n"],"metadata":{"id":"RG3WSjAIw0s9","executionInfo":{"status":"aborted","timestamp":1733337517928,"user_tz":-60,"elapsed":11,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]}]}