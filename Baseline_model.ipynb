{"cells":[{"cell_type":"markdown","metadata":{"id":"yxKOgcdFc8Wt"},"source":["# Ez lesz a baseline CNN model, amit mi készítünk és tanítunk"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"U-ZIw9YOW-ek"},"outputs":[],"source":["import os\n","from PIL import Image, ImageFile\n","from torchvision import datasets, transforms, models\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torch import nn, optim\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","import numpy as np\n","import seaborn as sns\n","import pandas as pd\n","\n","# Allow the loading of images even if they are corrupted\n","ImageFile.LOAD_TRUNCATED_IMAGES = True"]},{"cell_type":"markdown","metadata":{"id":"UpJxJQJE4jQM"},"source":["**Mount our drive folder and define the .zip path**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"pJkI_NZo4id5"},"outputs":[],"source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the path to the ZIP file in your Google Drive\n","zip_file_path = '/content/drive/MyDrive/ADATELEMZÉS_HF/top_100_species_images.zip'"]},{"cell_type":"markdown","metadata":{"id":"PGx3xwUYdkuU"},"source":["**Transformations** (converting from raw input data to processable data)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"IA5JoPO2e11b"},"outputs":[],"source":["from torchvision import transforms\n","\n","# Define transformations to be applied to each image (resizing, converting to tensor, normalizing)\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),  # Resize images to 288x288 pixels\n","    transforms.ToTensor(),  # Convert image to a tensor (used by PyTorch)\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])"]},{"cell_type":"markdown","metadata":{"id":"AyokoKjmW6Yb"},"source":["**Define our custom convolutional layer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lME5APbRW-w_"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","\n","# Convolutional module (Conv+ReLU+BatchNorm)\n","class Conv(nn.Module):\n","\n","    # Constructor gets in and output channels and stride\n","    def __init__(self, in_channels, channels, stride=1):\n","        super().__init__()\n","\n","        # Create 2D Convolution (3x3)\n","        self.conv = nn.Conv2d(in_channels, channels, kernel_size=3,\n","                              stride=stride, padding=1, bias=False)\n","\n","        # Create Batchnorm\n","        self.bn = nn.BatchNorm2d(channels)\n","\n","    # Overwrite forward\n","    def forward(self,x):\n","        # Call the layers in the proper order\n","        return self.bn(torch.relu(self.conv(x)))"]},{"cell_type":"markdown","metadata":{"id":"nuGdaY5CXHuT"},"source":["**Our convolutional network**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TK6tPeNjXnyH"},"outputs":[],"source":["class ConvNet(nn.Module):\n","    def __init__(self, base_channels=16, in_channels=3, num_classes=100):\n","        super().__init__()\n","\n","        # First two filters\n","        self.c11 = Conv(in_channels, base_channels)\n","        self.c12 = Conv(base_channels, base_channels)\n","\n","        # Downscale using strided convolution and expand channels\n","        self.d1 = Conv(base_channels, base_channels*2, 2)             # 128x128 --> 64x64\n","\n","        # Repeat this 4 times\n","        self.c21 = Conv(base_channels*2, base_channels*2)\n","        self.c22 = Conv(base_channels*2, base_channels*2)\n","        self.d2 = Conv(base_channels*2, base_channels*4, 2)             # 64x64 --> 32x32\n","\n","        self.c31 = Conv(base_channels*4, base_channels*4)\n","        self.c32 = Conv(base_channels*4, base_channels*4)\n","        self.d3 = Conv(base_channels*4, base_channels*8, 2)             # 32x32 --> 16x16\n","\n","        self.c41 = Conv(base_channels*8, base_channels*8)\n","        self.c42 = Conv(base_channels*8, base_channels*8)\n","        self.d4 = Conv(base_channels*8, base_channels*16, 2)             # 16x16 --> 8x8\n","\n","        # self.c51 = Conv(base_channels*16, base_channels*16)\n","        # self.c52 = Conv(base_channels*16, base_channels*16)\n","        # self.d5 = Conv(base_channels*16, base_channels*32, 2)\n","\n","        # Add two average pooling layers to reduce spatial dimensions to 1x1\n","        self.avgpool1 = nn.AvgPool2d(kernel_size=2, stride=2)  # 8x8 --> 4x4\n","        self.avgpool2 = nn.AvgPool2d(kernel_size=2, stride=2)  # 4x4 --> 2x2\n","        self.avgpool3 = nn.AvgPool2d(kernel_size=2, stride=2)  # 2x2 --> 1x1\n","\n","        # Final classifier layer using 1x1 convolution\n","        self.classifier = nn.Conv2d(base_channels*16, num_classes, kernel_size=1)\n","\n","    def forward(self, x):\n","        # Pass input through convolutional layers\n","        x = self.d1(self.c12(self.c11(x)))\n","        x = self.d2(self.c22(self.c21(x)))\n","        x = self.d3(self.c32(self.c31(x)))\n","        x = self.d4(self.c42(self.c41(x)))\n","        # x = self.d5(self.c52(self.c51(x)))\n","\n","        # Apply average pooling to reduce spatial dimensions\n","        # multikolineáris?\n","        x = self.avgpool1(x)\n","        x = self.avgpool2(x)\n","        x = self.avgpool3(x)\n","\n","        # Pass through classifier layer\n","        x = self.classifier(x)\n","\n","        return x.view(x.size(0), -1)  # Flatten output to (batch_size, num_classes)"]},{"cell_type":"markdown","metadata":{"id":"BCgKDrNoWgdA"},"source":["We have to get the class_id of a picture based on its observationID from the csv file"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fneIjqP0W0FB"},"outputs":[],"source":["#### Load the dataset from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/ADATELEMZÉS_HF/FungiCLEF2023_train_metadata_PRODUCTION.csv')\n","\n","#### Find the top 100 species by number of examples\n","top_100_species = df['species'].value_counts().head(100).index.tolist()\n","\n","#### Filter the dataframe to only include rows from the top 100 species\n","df_top_100 = df[df['species'].isin(top_100_species)]\n","\n","#### Create a new dataframe with only the 'image_path' and 'class_id' columns\n","top_100_pairs = df_top_100[['image_path', 'species']]\n","\n","#### Print the shape of the final dataframe\n","print(f\"Final dataframe shape: {top_100_pairs.shape}\")\n","\n","unique_species_ids = top_100_pairs['species'].nunique()\n","print(f\"Number of unique species: {unique_species_ids}\")\n","print(top_100_species)\n","\n","# Create a dictionary that maps each species to a unique integer label\n","species_labels = {species: index for index, species in enumerate(top_100_species)}\n","\n","print(species_labels)"]},{"cell_type":"markdown","metadata":{"id":"N4edGdhsoLU2"},"source":["# Defining a custom dataset for loading our data and labels"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qEgM5ITZoVFe"},"outputs":[],"source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import zipfile\n","\n","# Define a custom dataset class for loading our images and labels\n","class CustomDataset(Dataset):\n","    def __init__(self, zip_file_path, top_100_df, transform=None):\n","        self.zip_file_path = zip_file_path\n","        self.top_100_df = top_100_df\n","        self.transform = transform\n","        self.image_paths = []  # List to store image paths\n","        self.labels = []       # List to store image labels\n","        image_count = 0\n","        label_count = 0\n","        iter = 0\n","\n","        print(\"Zip folder path:\", zip_file_path)  # Print the folder path\n","\n","        # Open the ZIP file\n","        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n","            # Get the name of the folder\n","            folder_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n","            print(\"Folder name:\", folder_name)  # Print the subfolder name\n","\n","            # Loop through each file in the ZIP file\n","            for file_info in zip_file.infolist():\n","                file_name = file_info.filename\n","                # print(\"File name:\", file_name)  # Print the file name\n","\n","                if file_name.startswith(folder_name + '/') and file_name.endswith('.JPG') and not file_name.startswith('__MACOSX/'):\n","                    #print(\"Matched file:\", file_name)  # Print the matched file name\n","\n","                    if(iter % 10 == 0):\n","                        # Get the image path after the '/' symbol\n","                        image_path_after_slash = os.path.basename(file_name)\n","                        #print(\"Needed name: \", image_path_after_slash)\n","\n","                        # Extract the label from the file name\n","                        label = species_labels.get(self.top_100_df.loc[self.top_100_df['image_path'] == image_path_after_slash, 'species'].values[0])\n","                        # print(\"Extracted label:\", label)  # Print the extracted label\n","\n","                        self.image_paths.append(file_name)  # Store the image path within the ZIP file\n","                        image_count += 1\n","                        self.labels.append(label)  # Store the corresponding label\n","                        label_count += 1\n","\n","                    iter = iter + 1\n","\n","                    if(label_count % 100 == 0):\n","                      print(label_count)\n","\n","        # print(\"Image paths:\", self.image_paths)\n","        # print(\"Labels:\", self.labels)\n","        print(len(set(self.labels)))\n","        print(\"ZIP file path:\", zip_file_path)  # Print the ZIP file path\n","        print(\"Subfolder name:\", folder_name)  # Print the subfolder name\n","        print(\"Image count:\", image_count)\n","        print(\"Label count:\", label_count)\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Open the ZIP file and read the image\n","        with zipfile.ZipFile(self.zip_file_path, 'r') as zip_file:\n","            img_path = self.image_paths[idx]\n","            image = Image.open(zip_file.open(img_path)).convert(\"RGB\")\n","            label = self.labels[idx]\n","\n","            if self.transform:\n","                image = self.transform(image)\n","\n","        return image, label"]},{"cell_type":"markdown","metadata":{"id":"wTTLo8iQ4Hj1"},"source":["**Initialize dataset and dataloader**"]},{"cell_type":"code","execution_count":null,"metadata":{"collapsed":true,"id":"-usFZtts4QJ1"},"outputs":[],"source":["# Create dataset and dataloader objects for the training dataset\n","dataset = CustomDataset(zip_file_path=zip_file_path, top_100_df=top_100_pairs, transform=transform)\n","\n","# Create DataLoaders to efficiently load data in batches\n","train_loader = DataLoader(dataset, batch_size=16, shuffle=True)\n"]},{"cell_type":"markdown","metadata":{"id":"lYZJOLfVv8s5"},"source":["**Function to instantiate Netzwerk**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y45HdHndvphU"},"outputs":[],"source":["# Check if CUDA is available and set the device\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","haveCuda = torch.cuda.is_available()\n","print(haveCuda)\n","\n","# Instantiate network and convert it to CUDA\n","def createNet():\n","    net = ConvNet()\n","    if haveCuda:\n","        net = net.cuda()\n","    return net\n","\n","net = createNet()"]},{"cell_type":"markdown","metadata":{"id":"kgrxy33bwRnk"},"source":["**Loss**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"cK4u2I59wSHy"},"outputs":[],"source":["def createLoss():\n","    return nn.CrossEntropyLoss()"]},{"cell_type":"markdown","metadata":{"id":"UjPUTuuMwxWh"},"source":["**Optimizer**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"VgCtU0juw06h"},"outputs":[],"source":["from torch import optim\n","\n","# Weight decay is the relative weight of the L2 regularization term\n","def createOptimizer():\n","    return optim.Adam(net.parameters(), lr=1e-5, weight_decay=1e-6)"]},{"cell_type":"markdown","metadata":{"id":"oQ7_PHbwxN2n"},"source":["**Learning rate scheduler (we will adjust the actual learning rate gradually)**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-NHZL7Uzw3B8"},"outputs":[],"source":["# Number of epochs (times iterating through the whole dataset)\n","numEpoch = 5\n","\n","# Cosine annealing learning rate scheduler - in 50 epochs the lr will become 0.01\n","def createScheduler():\n","    return optim.lr_scheduler.CosineAnnealingLR(optimizer,numEpoch,eta_min=1e-2)"]},{"cell_type":"markdown","source":["Progress bar to make the training more interesting"],"metadata":{"id":"wzpdufInnEYX"}},{"cell_type":"code","source":["from IPython.display import HTML, display\n","\n","def progress(value, max=100):\n","    return HTML(\"\"\"\n","        <progress\n","            value='{value}'\n","            max='{max}',\n","            style='width: 100%'\n","        >\n","            {value}\n","        </progress>\n","    \"\"\".format(value=value, max=max))"],"metadata":{"id":"Jqm5ZjG3nJ9v"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"yxF5lr6qi47u"},"outputs":[],"source":["net = createNet()\n","loss = createLoss()\n","optimizer = createOptimizer()\n","scheduler = createScheduler()"]},{"cell_type":"markdown","source":["**Define a training for a given model and dataset**"],"metadata":{"id":"qY9BzkBGqumy"}},{"cell_type":"code","source":["def train_epoch(model, dataloader, optimizer, criterion, device):\n","    model.train()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    total = 0\n","\n","    for batch_idx, (inputs, labels) in enumerate(dataloader):\n","        print(\"Itt vagyok:\", batch_idx + 1)\n","        inputs = inputs.to(device)\n","        labels = labels.to(device)\n","\n","        optimizer.zero_grad()\n","        outputs = model(inputs)\n","        _, preds = torch.max(outputs, 1)\n","        loss = criterion(outputs, labels)\n","\n","        loss.backward()\n","        optimizer.step()\n","\n","        running_loss = running_loss + (loss.item() * inputs.size(0))\n","        running_corrects = running_corrects + torch.sum(preds == labels.data).item()\n","        total = total + labels.size(0)\n","        print(\"Total correct:\", running_corrects)\n","        print(\"Total:\", total)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = running_corrects.double() / total\n","\n","    return epoch_loss, epoch_acc"],"metadata":{"id":"UK1rjbORORcf"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define evaluation for a given model and dataset (no change in gradients, just evaluating the model)**"],"metadata":{"id":"yGeFrzTJO4Lq"}},{"cell_type":"code","source":["def evaluate(model, dataloader, criterion, device):\n","    model.eval()\n","    running_loss = 0.0\n","    running_corrects = 0\n","    total = 0\n","\n","    with torch.no_grad():\n","        for batch_idx, (inputs, labels) in enumerate(dataloader):\n","            inputs = inputs.to(device)\n","            labels = labels.to(device)\n","\n","            outputs = model(inputs)\n","            _, preds = torch.max(outputs, 1)\n","            loss = criterion(outputs, labels)\n","\n","            running_loss = running_loss + (loss.item() * inputs.size(0))\n","            running_corrects = running_corrects + torch.sum(preds == labels.data).item()\n","            total = total + labels.size(0)\n","\n","    epoch_loss = running_loss / total\n","    epoch_acc = running_corrects.double() / total\n","\n","    return epoch_loss, epoch_acc"],"metadata":{"id":"iaFxfnFkO3Cb"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Reset weights on a model (needed between epochs)**"],"metadata":{"id":"gCRRdaozPO-J"}},{"cell_type":"code","source":["def reset_weights(m):\n","    if isinstance(m, torch.nn.Conv2d) or isinstance(m, torch.nn.Linear):\n","        m.reset_parameters()"],"metadata":{"id":"4sveQ1KKPTxA"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["**Define k-fold training with a given number of epochs in each split**"],"metadata":{"id":"Lk0ceW4DPYX1"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"Pv7v9fdGkfNG"},"outputs":[],"source":["from sklearn.model_selection import KFold\n","from torch.utils.data import SubsetRandomSampler\n","\n","def train_kfold(model, dataset, batch_size, num_folds, num_epochs, optimizer, criterion, device):\n","    # Create a KFold object\n","    kfold = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","\n","    # Initialize lists to store the training and validation losses and accuracies\n","    train_losses = []\n","    val_losses = []\n","    train_accuracies = []\n","    val_accuracies = []\n","\n","    # Perform cross-validation\n","    for fold, (train_idx, val_idx) in enumerate(kfold.split(dataset)):\n","        print(f\"Fold {fold+1}\")\n","\n","        # Create samplers for training and validation indices\n","        train_sampler = SubsetRandomSampler(train_idx)\n","        val_sampler = SubsetRandomSampler(val_idx)\n","\n","        # Create DataLoaders for training and validation sets\n","        train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler)\n","        val_loader = DataLoader(dataset, batch_size=batch_size, sampler=val_sampler)\n","\n","        # Train the model for the current fold\n","        for epoch in range(num_epochs):\n","            train_loss, train_acc = train_epoch(model, train_loader, optimizer, criterion, device)\n","            val_loss, val_acc = evaluate(model, val_loader, criterion, device)\n","\n","            train_losses.append(train_loss)\n","            val_losses.append(val_loss)\n","            train_accuracies.append(train_acc)\n","            val_accuracies.append(val_acc)\n","\n","            print(f\"Epoch {epoch+1}: Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}, Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n","\n","        # Reset the model weights for the next fold\n","        model.apply(reset_weights)\n","\n","    return train_losses, val_losses, train_accuracies, val_accuracies"]},{"cell_type":"markdown","source":["# Training loop"],"metadata":{"id":"gdb0PEmZqrqY"}},{"cell_type":"code","source":["# Train the model using k-fold cross-validation\n","num_folds = 5\n","num_epochs = 1\n","batch_size = 16\n","\n","train_losses, val_losses, train_accuracies, val_accuracies = train_kfold(\n","    net, dataset, batch_size=16, num_folds=10, num_epochs=3, optimizer=optimizer, criterion= loss, device=device)\n","\n","# Save the model\n","torch.save(net.state_dict(), '/content/drive/MyDrive/ADATELEMZÉS_HF/trained_model.pth')"],"metadata":{"id":"ayJKKIkmqtW1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.metrics import f1_score, classification_report, confusion_matrix\n","\n","y_pred = model.predict(X_test)\n","\n","y_pred_classes = y_pred.argmax(axis=1)\n","\n","macro_f1 = f1_score(y_test, y_pred_classes, average='macro')\n","print(f\"Macro F1 score: {macro_f1:.4f}\")\n","\n","print(\"Classification Report:\\n\", classification_report(y_test, y_pred_classes))"],"metadata":{"id":"Y8U9KTiouZ8b"},"execution_count":null,"outputs":[]}],"metadata":{"colab":{"provenance":[],"gpuType":"T4"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}