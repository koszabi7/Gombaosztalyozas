{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyM/vv1ADFCncvv66cJGIBjJ"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":16,"metadata":{"id":"ovPcOWq6m6aT","executionInfo":{"status":"ok","timestamp":1733337673365,"user_tz":-60,"elapsed":634,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"outputs":[],"source":["# Import necessary libraries\n","import zipfile\n","import os\n","import numpy as np\n","from tensorflow.keras.preprocessing.image import img_to_array, load_img\n","from tensorflow.keras.applications.resnet50 import ResNet50, preprocess_input\n","from tensorflow.keras.layers import Dense, GlobalAveragePooling2D\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score\n","import tensorflow as tf\n","from sklearn.preprocessing import LabelEncoder"]},{"cell_type":"code","source":["import os\n","from PIL import Image, ImageFile\n","from torchvision import datasets, transforms, models\n","import torch\n","from torch.utils.data import DataLoader, Dataset\n","from torch import nn, optim\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n","import numpy as np\n","import seaborn as sns\n","import pandas as pd"],"metadata":{"id":"9fHrJQJGnA_1","executionInfo":{"status":"ok","timestamp":1733337674020,"user_tz":-60,"elapsed":10,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""],"metadata":{"id":"JuITXagm73pF","executionInfo":{"status":"ok","timestamp":1733337674021,"user_tz":-60,"elapsed":9,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":18,"outputs":[]},{"cell_type":"code","source":["from google.colab import drive\n","\n","# Mount Google Drive\n","drive.mount('/content/drive')\n","\n","# Define the path to the ZIP file in your Google Drive\n","zip_file_path = '/content/drive/MyDrive/ADATELEMZÉS_HF/top_100_species_images.zip'"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"hzcozrsJnC5q","executionInfo":{"status":"ok","timestamp":1733337676276,"user_tz":-60,"elapsed":2263,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}},"outputId":"b9962b65-7db1-4062-fd98-f77368e8df3c"},"execution_count":19,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["#### Load the dataset from CSV file\n","df = pd.read_csv('/content/drive/MyDrive/ADATELEMZÉS_HF/FungiCLEF2023_train_metadata_PRODUCTION.csv')\n","\n","#### Find the top 100 species by number of examples\n","top_100_species = df['species'].value_counts().head(100).index.tolist()\n","\n","#### Filter the dataframe to only include rows from the top 100 species\n","df_top_100 = df[df['species'].isin(top_100_species)]\n","\n","#### Create a new dataframe with only the 'image_path' and 'poisonous' columns\n","top_100_pairs = df_top_100[['image_path', 'poisonous']]\n","top_100_pairs['poisonous'] = top_100_pairs['poisonous'].astype(float)\n","\n","#### Print the shape of the final dataframe\n","print(f\"Final dataframe shape: {top_100_pairs.shape}\")\n","\n","unique_poisonous_types = top_100_pairs['poisonous'].nunique()\n","print(f\"Number of unique types: {unique_poisonous_types}\")\n","print(top_100_species)\n","\n","# Create a dictionary that maps each species to a unique integer label\n","#species_labels = {poisonous: index for index, poisonous in top_100_poisonous}\n","poisinous_labels = df_top_100.groupby('species')['poisonous'].first().to_dict()\n","\n","print(poisinous_labels)\n","print(top_100_pairs.head())"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"guiGsuxhnEn-","executionInfo":{"status":"error","timestamp":1733337683827,"user_tz":-60,"elapsed":7557,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}},"outputId":"fd37d3c6-f723-4e30-e782-27210b537ec1","collapsed":true},"execution_count":20,"outputs":[{"output_type":"stream","name":"stderr","text":["\n","KeyboardInterrupt\n","\n"]}]},{"cell_type":"code","source":["from torchvision import transforms\n","\n","# Define transformations to be applied to each image (resizing, converting to tensor, normalizing)\n","transform = transforms.Compose([\n","    transforms.Resize((128, 128)),  # Resize images to 288x288 pixels\n","    transforms.ToTensor(),  # Convert image to a tensor (used by PyTorch)\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406],\n","                         std=[0.229, 0.224, 0.225])\n","])"],"metadata":{"id":"yokA_ocDnIoX","executionInfo":{"status":"aborted","timestamp":1733337683828,"user_tz":-60,"elapsed":13,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import os\n","from PIL import Image\n","from torch.utils.data import Dataset\n","import zipfile\n","\n","# Define a custom dataset class for loading our images and labels\n","class CustomDataset(Dataset):\n","    def __init__(self, zip_file_path, top_100_df, transform=None):\n","        self.zip_file_path = zip_file_path\n","        self.top_100_df = top_100_df\n","        self.transform = transform\n","        self.image_paths = []  # List to store image paths\n","        self.labels = []       # List to store image labels\n","        image_count = 0\n","        label_count = 0\n","        iter = 0\n","\n","        print(\"Zip folder path:\", zip_file_path)  # Print the folder path\n","\n","        # Open the ZIP file\n","        with zipfile.ZipFile(zip_file_path, 'r') as zip_file:\n","            # Get the name of the folder\n","            folder_name = os.path.splitext(os.path.basename(zip_file_path))[0]\n","            print(\"Folder name:\", folder_name)  # Print the subfolder name\n","\n","            # Loop through each file in the ZIP file\n","            for file_info in zip_file.infolist():\n","                file_name = file_info.filename\n","                # print(\"File name:\", file_name)  # Print the file name\n","\n","                if file_name.startswith(folder_name + '/') and file_name.endswith('.JPG') and not file_name.startswith('__MACOSX/'):\n","                    #print(\"Matched file:\", file_name)  # Print the matched file name\n","\n","                    if(iter % 100 == 0):\n","                        # Get the image path after the '/' symbol\n","                        image_path_after_slash = os.path.basename(file_name)\n","                        #print(\"Needed name: \", image_path_after_slash)\n","\n","                        # Extract the label from the file name\n","                        label = self.top_100_df.loc[self.top_100_df['image_path'] == image_path_after_slash, 'poisonous'].values[0]\n","                        print(\"Extracted label:\", label)  # Print the extracted label\n","\n","                        self.image_paths.append(file_name)  # Store the image path within the ZIP file\n","                        image_count += 1\n","                        self.labels.append(label)  # Store the corresponding label\n","                        label_count += 1\n","\n","                    iter = iter + 1\n","\n","                    if(label_count % 100 == 0):\n","                      print(label_count)\n","\n","        # print(\"Image paths:\", self.image_paths)\n","        # print(\"Labels:\", self.labels)\n","        print(len(set(self.labels)))\n","        print(\"ZIP file path:\", zip_file_path)  # Print the ZIP file path\n","        print(\"Subfolder name:\", folder_name)  # Print the subfolder name\n","        print(\"Image count:\", image_count)\n","        print(\"Label count:\", label_count)\n","        print(self.labels[:10])\n","        #self.labels = [float(label) for label in self.labels]\n","        #print(\"Labels dtype:\", self.labels.dtype)\n","        #print(\"Unique labels:\", torch.unique(self.labels))\n","\n","    def __len__(self):\n","        return len(self.image_paths)\n","\n","    def __getitem__(self, idx):\n","        # Open the ZIP file and read the image\n","        with zipfile.ZipFile(self.zip_file_path, 'r') as zip_file:\n","            img_path = self.image_paths[idx]\n","            image = Image.open(zip_file.open(img_path)).convert(\"RGB\")\n","            label = self.labels[idx]\n","\n","            if self.transform:\n","                image = self.transform(image)\n","\n","            if image is None or label is None:\n","                print(f\"Found None at index {idx}: image={image}, label={label}\")\n","\n","        return image, label"],"metadata":{"id":"DoDLy0gMnKiY","executionInfo":{"status":"aborted","timestamp":1733337683829,"user_tz":-60,"elapsed":13,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create dataset and dataloader objects for the training dataset\n","dataset = CustomDataset(zip_file_path=zip_file_path, top_100_df=top_100_pairs, transform=transform)\n","\n","# Create DataLoaders to efficiently load data in batches\n","train_loader = DataLoader(dataset, batch_size=16, shuffle=True)"],"metadata":{"id":"M6_-LUD0nMUp","executionInfo":{"status":"aborted","timestamp":1733337683829,"user_tz":-60,"elapsed":13,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for images, labels in train_loader:\n","    print(f\"Images shape: {images.shape}, Labels shape: {labels.shape}\")\n","    labels = labels.to(torch.float32)\n","    break"],"metadata":{"id":"FVhvtUk24Lj5","executionInfo":{"status":"aborted","timestamp":1733337683829,"user_tz":-60,"elapsed":13,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(\"Labels dtype:\", labels.dtype)\n","print(\"Unique labels:\", torch.unique(labels))"],"metadata":{"id":"9Z2UXrKC5J1I","executionInfo":{"status":"aborted","timestamp":1733337683829,"user_tz":-60,"elapsed":13,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Arp-ODnc6LmL","executionInfo":{"status":"aborted","timestamp":1733337683829,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torchvision.models import efficientnet_b0, EfficientNet_B0_Weights\n","\n","# Define the model using pre-trained ResNet-50\n","class MushroomClassifier(nn.Module):\n","    def __init__(self, num_classes=1):\n","        super(MushroomClassifier, self).__init__()\n","        # Use the updated weights argument\n","        weights = EfficientNet_B0_Weights.IMAGENET1K_V1\n","        self.model = efficientnet_b0(weights=weights)\n","        self.model.classifier = nn.Sequential(\n","            nn.Dropout(0.2),  # Optional dropout\n","            nn.Linear(self.model.classifier[1].in_features, num_classes)\n","        )\n","\n","    def forward(self, x):\n","        return self.model(x)\n","\n","# Instantiate the model and move it to GPU if available\n","num_classes = 1\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model = MushroomClassifier(num_classes=num_classes)\n","model.to(device)\n","print(\"Model initialized successfully.\")"],"metadata":{"id":"ZqqIXmrunN_D","executionInfo":{"status":"aborted","timestamp":1733337683829,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Set up the loss function and optimizer\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.Adam(model.parameters(), lr=0.001)\n"],"metadata":{"id":"KMyH67YgnQnT","executionInfo":{"status":"aborted","timestamp":1733337683830,"user_tz":-60,"elapsed":13,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from sklearn.model_selection import KFold\n","\n","\n","# Number of folds\n","num_folds = 5\n","\n","# Initialize KFold\n","kf = KFold(n_splits=num_folds, shuffle=True, random_state=42)\n","\n","# Placeholder for cross-validation results\n","fold_results = []\n","\n","# Define the number of epochs\n","num_epochs = 10"],"metadata":{"id":"-_5gHZaPnSfF","executionInfo":{"status":"aborted","timestamp":1733337683830,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.nn import BCEWithLogitsLoss\n","from torch.optim import Adam\n","from torch.optim.lr_scheduler import CosineAnnealingLR\n","\n","# Cross-validation loop\n","for fold, (train_idx, val_idx) in enumerate(kf.split(dataset)):\n","    print(f\"\\nFold {fold + 1}/{num_folds}\")\n","\n","    # Split dataset into training and validation sets for this fold\n","    print(\"Splitting dataset...\")\n","    train_subset = torch.utils.data.Subset(dataset, train_idx)\n","    val_subset = torch.utils.data.Subset(dataset, val_idx)\n","\n","    # Create DataLoaders\n","    print(\"Creating DataLoaders...\")\n","    train_loader = DataLoader(train_subset, batch_size=1, shuffle=True)\n","    val_loader = DataLoader(val_subset, batch_size=1, shuffle=False)\n","\n","    # Initialize the model\n","    print(\"Initializing the model...\")\n","    model = MushroomClassifier(num_classes=1).to(device)\n","\n","    # Define loss, optimizer, and scheduler\n","    print(\"Setting up loss function, optimizer, and scheduler...\")\n","    criterion = BCEWithLogitsLoss()\n","    optimizer = Adam(model.parameters(), lr=1e-3)\n","    scheduler = CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-5)\n","\n","    # Training and validation loop for this fold\n","    for epoch in range(num_epochs):\n","        print(f\"\\n--- Epoch {epoch + 1}/{num_epochs} ---\")\n","        model.train()\n","        train_loss = 0.0\n","\n","        # Training\n","        for images, labels in train_loader:\n","            images, labels = images.to(device), labels.to(device).float()\n","\n","            # Forward pass\n","            print(\"Forward pass...\")\n","            outputs = model(images).squeeze(1)\n","            loss = criterion(outputs, labels)\n","\n","            # Backward pass\n","            print(\"Backward pass...\")\n","            optimizer.zero_grad()\n","            loss.backward()\n","            optimizer.step()\n","\n","            train_loss += loss.item()\n","\n","        # Scheduler step\n","        print(\"Scheduler step...\")\n","        scheduler.step()\n","\n","        # Validation\n","        print(\"Validation...\")\n","        model.eval()\n","        val_loss = 0.0\n","        all_preds = []\n","        all_labels = []\n","\n","        with torch.no_grad():\n","            for images, labels in val_loader:\n","                images, labels = images.to(device), labels.to(device).float()\n","\n","                outputs = model(images).squeeze()\n","                loss = criterion(outputs, labels)\n","\n","                val_loss += loss.item()\n","\n","                # Convert logits to predictions\n","                preds = torch.sigmoid(outputs) > 0.5\n","                all_preds.extend(preds.cpu().numpy())\n","                all_labels.extend(labels.cpu().numpy())\n","\n","        # Calculate metrics\n","        val_loss /= len(val_loader)\n","        train_loss /= len(train_loader)\n","        f1 = f1_score(all_labels, all_preds)\n","        precision = precision_score(all_labels, all_preds)\n","        recall = recall_score(all_labels, all_preds)\n","\n","        print(f\"Epoch {epoch + 1} | Train Loss: {train_loss:.4f} | Val Loss: {val_loss:.4f} | F1: {f1:.4f} | Precision: {precision:.4f} | Recall: {recall:.4f}\")\n","\n","    # Log metrics for this fold\n","    fold_results.append({\n","        \"fold\": fold + 1,\n","        \"f1_score\": f1,\n","        \"precision\": precision,\n","        \"recall\": recall,\n","        \"val_loss\": val_loss\n","    })\n"],"metadata":{"id":"2FShCM80nUFQ","executionInfo":{"status":"aborted","timestamp":1733337683830,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Summary of cross-validation results\n","results_df = pd.DataFrame(fold_results)\n","print(\"\\nCross-validation results:\")\n","print(results_df)\n","print(\"\\nAverage F1 Score:\", results_df[\"f1_score\"].mean())"],"metadata":{"id":"7LnDnCf7nYAg","executionInfo":{"status":"aborted","timestamp":1733337683830,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"7r6_NOk8nYkc","executionInfo":{"status":"aborted","timestamp":1733337683830,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"ppPPfs5lnaaB","executionInfo":{"status":"aborted","timestamp":1733337683830,"user_tz":-60,"elapsed":11,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"KXoAAPUSncWc","executionInfo":{"status":"aborted","timestamp":1733337683831,"user_tz":-60,"elapsed":12,"user":{"displayName":"Házifeladat Adatelemzés","userId":"03777937392997268846"}}},"execution_count":null,"outputs":[]}]}